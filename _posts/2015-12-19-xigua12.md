---
layout: post
title: 西瓜书笔记（七）
tags: [机器学习, 读书笔记, 总结]
category: 技术
mathjax: true
---
>既然开始学习机器学习，那么肯定不能少了经典的西瓜书，作为入门经典教材，讲解的通俗易懂，而且较为系统。工作之余，我也决定读一遍西瓜书，可能大部分内容都有所了解了，这个系列的博客就再总结些自己认为重要的。

# 第12章 计算学习理论
## 12.1 PAC（概率近似正确）学习
设置某个置信度$\delta$,来近似学习一个目标概念，然后讨论在这个置信度下学习算法的能否学习等概念。
## 12.2 有限空间假设
在可分情况下，到底多少样例才能学得目标概念的有效近似呢？
$$m\geq\frac1\epsilon(\ln\left|\mathcal{H}\right|+\ln{\frac1\delta})$$
其中$\delta$为置信度，$\epsilon$为模型误差上限，$\mathcal{H}$为假设空间，即学习算法所有可能概念的集合。
## 12.3 VC维
1. 几个概念
用来度量空间的复杂度，首先要明确几个概念：

	* 增长函数
 给定假设空间$\mathcal{H}$和示例集$D=\{x_1,x_2,\ldots,x_m\}$,$\mathcal{H}$中每个假设h都能对D中示例赋予标记，标记结果可表示为：
 $$h|_D=\{h(x_1),h(x_2),\ldots,h(x_m)\}$$
 随着m的增大，$\mathcal{H}$中所有假设对D中的示例所能赋予的标记的可能结果数也会增大。那么假设空间$\mathcal{H}$的增长函数$\Pi_\mathcal{H}(m)$为
 $$\Pi_\mathcal{H}(m)=\max_{\{x_1,\ldots,x_m\}\subseteq\chi}\left|\{h(x_1),\ldots,h(x_m)|h\in\mathcal{H}\right|$$
 增长函数表示假设空间$\mathcal{H}$对m个示例所能赋予标记的最大可能结果数。假设空间$\mathcal{H}$不同假设对示例赋予的标记可能相同，也可能不同；但对m个示例，最多有$2^m$个可能结果（二分类前提）。

	* 打散
若假设空间$\mathcal{H}$能实现示例集D上的所有对分，即$\Pi_\mathcal{H}(m)=2^m$,则称示例集D能被假设空间$\mathcal{H}$*打散*。

2. VC维
$$VC(\mathcal{H})=\max\{m:\Pi_\mathcal{H}(m)=2^m\}$$
**VC维的定义与数据分布D无关**