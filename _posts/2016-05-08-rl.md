---
layout: post
title: 强化学习初窥
tags: [强化学习, 摘要, 总结]
category: 技术
mathjax: true
---
>从alphago开始，就对强化学习感兴趣了。以为目前的浅薄认识，我认为这才是最类似人类智能形成的方式——“尝试——反馈”，即如果这样做对我有利，我就大概率这么做。因为我本身也是个游戏狂热爱好者，OpenAI已经可以1V1打赢Dendi了，而我目前玩的炉石，他可以获得的信息实时性不强，而且很容易产生结构化输出。我认为很有可能通过强化学习，得到一个很强的bot，所以这个系列的博客将总结强化学习相关内容，争取从简单的围棋出发，自己也开发出一个“外挂”横扫天梯。

## K摇臂赌博机
与一般监督学习不同，强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：最大化单步奖赏，这与监督学习有显著不同，因为没有训练数据告诉机器应当做哪个动作。考虑一个动作的奖赏值是来自于一个概率分布（一次尝试并不能确切地获得平均奖赏值）。分为：
* 仅探索：将所有的尝试机会平均分配给每个摇臂
* 仅利用：按下目前最有的摇臂。
这两种方法都难以使最终的累积奖赏最大化，这就是强化学习所面临的“探索-利用窘境”
1. $\epsilon$-贪心
$\epsilon$-贪心：每次尝试有$\epsilon$概率进行探索，有$1-\epsilon$概率去利用。
2. Softmax
Softmax算法基于当前已知的摇臂平均奖励来对探索和利用进行折中，平均奖励更高的摇臂，被选中的概率也就更高。

## 有模型学习
考虑多步强化学习任务，暂且先假定任务对应的Markov决策过程四元组均为已知，即机器已经对环境进行了建模，能在机器内部模拟出与环境相同或近似的状况。
## 免模型学习
现实任务中，往往很难获知四元组。
1. MC强化学习
2. 时序差分学习

## 值函数学习

## 模仿学习

能从范例中学习。
1. 直接模仿学习
基于累积奖赏来学习很多步之前的合适决策非常困难，而直接模仿人类专家的“状态-动作对”。
2. 逆强化学习
欲使机器做出与范例一致的行为，等价于在某个奖赏函数的环境中求解最优策略，该最优策略所产生的轨迹与范例数据一致，那么我们可以寻找某种奖赏函数使得范例数据时最优的，然后即可使用这个奖赏函数来训练强化学习策略。

## DQL
DRL是将深度学习（DL）与强化学习（RL）结合，直接从高维原始数据学习控制策略。而DQN是DRL的其中一种算法，它要做的就是将卷积神经网络（CNN）和Q-Learning结合起来，CNN的输入是原始图像数据（作为状态State），输出则是每个动作Action对应的价值评估Value Function（Q值）。

二、DL与RL结合的问题
DL需要大量带标签的样本进行监督学习；RL只有reward返回值，而且伴随着噪声，延迟（过了几十毫秒才返回），稀疏（很多State的reward是0）等问题；
DL的样本独立；RL前后state状态相关；
DL目标分布固定；RL的分布一直变化，比如你玩一个游戏，一个关卡和下一个关卡的状态分布是不同的，所以训练好了前一个关卡，下一个关卡又要重新训练；
过往的研究表明，使用非线性网络表示值函数时出现不稳定等问题。

DQN是第一个将深度学习模型与强化学习结合在一起从而成功地直接从高维的输入学习控制策略。

创新点：

基于Q-Learning构造Loss Function（不算很新，过往使用线性和非线性函数拟合Q-Table时就是这样做）。
通过experience replay（经验池）解决相关性及非静态分布问题；
使用TargetNet解决稳定性问题。
优点：

算法通用性，可玩不同游戏；
End-to-End 训练方式；
可生产大量样本供监督学习。
缺点：

无法应用于连续动作控制；
只能处理只需短时记忆问题，无法处理需长时记忆问题（后续研究提出了使用LSTM等改进方法）；
CNN不一定收敛，需精良调参。