---
layout: post
title: 强化学习初窥
tags: [强化学习, 摘要, 总结]
category: 技术
mathjax: true
---
>从alphago开始，就对强化学习感兴趣了。以为目前的浅薄认识，我认为这才是最类似人类智能形成的方式——“尝试——反馈”，即如果这样做对我有利，我就大概率这么做。因为我本身也是个游戏狂热爱好者，OpenAI已经可以1V1打赢Dendi了，而我目前玩的炉石，他可以获得的信息实时性不强，而且很容易产生结构化输出。我认为很有可能通过强化学习，得到一个很强的bot，所以这个系列的博客将总结强化学习相关内容，争取从简单的围棋出发，自己也开发出一个“外挂”横扫天梯。

## K摇臂赌博机
与一般监督学习不同，强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：最大化单步奖赏，这与监督学习有显著不同，因为没有训练数据告诉机器应当做哪个动作。考虑一个动作的奖赏值是来自于一个概率分布（一次尝试并不能确切地获得平均奖赏值）。分为：
* 仅探索：将所有的尝试机会平均分配给每个摇臂
* 仅利用：按下目前最有的摇臂。
这两种方法都难以使最终的累积奖赏最大化，这就是强化学习所面临的“探索-利用窘境”
1. $\epsilon$-贪心
$\epsilon$-贪心：每次尝试有$\epsilon$概率进行探索，有$1-\epsilon$概率去利用。
2. Softmax
Softmax算法基于当前已知的摇臂平均奖励来对探索和利用进行折中，平均奖励更高的摇臂，被选中的概率也就更高。

## 有模型学习
考虑多步强化学习任务，暂且先假定任务对应的Markov决策过程四元组均为已知，即机器已经对环境进行了建模，能在机器内部模拟出与环境相同或近似的状况。
## 免模型学习
现实任务中，往往很难获知四元组。
1. MC强化学习
2. 时序差分学习

## 值函数学习

## 模仿学习

能从范例中学习。
1. 直接模仿学习
基于累积奖赏来学习很多步之前的合适决策非常困难，而直接模仿人类专家的“状态-动作对”。
2. 逆强化学习
欲使机器做出与范例一致的行为，等价于在某个奖赏函数的环境中求解最优策略，该最优策略所产生的轨迹与范例数据一致，那么我们可以寻找某种奖赏函数使得范例数据时最优的，然后即可使用这个奖赏函数来训练强化学习策略。

## DQL
DRL是将深度学习（DL）与强化学习（RL）结合，直接从高维原始数据学习控制策略。而DQN是DRL的其中一种算法，它要做的就是将卷积神经网络（CNN）和Q-Learning结合起来，CNN的输入是原始图像数据（作为状态State），输出则是每个动作Action对应的价值评估Value Function（Q值）。