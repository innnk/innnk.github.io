---
layout: post
title: ML Yearning（21~25章）
tags: [机器学习, 读书笔记, 总结]
category: 技术
mathjax: true
---
>我的DL启蒙导师Andrew Ng（吴恩达）又出新书了，虽然是草稿，但完全免费（圈起来，要考）也实在太良心了！！！《九阴真经》订阅就送有木有！！。我是第一时间就订阅，每有新的更新就会直接收到邮件。目前收到了前几章，读取来感觉又是太良心了！！！全是工程实践的干货有木有！！对于项目经验欠缺的我来说，真实如获至宝。这个系列的博客我将分享自己读后笔记，欢迎订阅！

## 21.偏差和方差举例
* 过拟合：训练错误率 = 1%；开发错误率 = 11%
* 欠你和：训练错误率 = 15%；开发错误率 = 16%
* 大侠请重新来过：训练错误率 = 15%；开发错误率 = 30%
* 成功：训练错误率 = 0.5%；开发错误率 = 1%

## 22.与最优错误率比较
假设你正在构建一个语音识别系统，并发现 14% 的音频片段有太多的背景噪声，或者太难以理解，导致即使是人类也无法识别出所说的内容。在这种情况下，即使是“最优”的语音识别系统也可能有约为 14% 的误差。
    偏差 = 最佳误差率（“不可避免偏差”）+ 可避免的偏差

## 23.处理偏差和方差
处理偏差和方差问题最简单的形式：
* 如果具有较高的可避免偏差，那么加大模型的规模（例如通过添加层/神经元数量来增加神经网络的大小）。
* 如果具有较高的方差，那么向训练集增加数据。
如果你的算法含有了一个精心设计的正则化方法，通常可以安全地加大模型的规模，而不会增加过拟合风险。避免使用更大模型的唯一原因就是这将使得计算代价变大。

## 24.偏差和方差间的权衡
大部分对学习算法进行的更改中，有一些能够减少偏差，但代价是增大方差，反之亦然。于是在偏差和方差之间就产生了“权衡”。
在现代，我们往往能够获取充足的数据，并且可以使用非常大的神经网络（深度学习）。因此，这种权衡的情况比较少，并且现在有更多的选择可以在不损害方差的情况下减少偏差，反之亦然。

## 25.减少可避免偏差的技术
如果你的学习算法存在着很高的可避免偏差，你可能会尝试下面的技术：
加大模型规模（例如神经元/层的数量）：这项技术能够使算法更好地拟合训练集，从而减少偏差。当你发现这样做会增大方差时，加入正则化，这可以抵消方差的增加。
根据误差分析结果修改输入特征：假设误差分析的结果鼓励你创建额外的特征，从而帮助算法消除某个特定类别的误差。（我们会在接下来的章节深入讨论这个话题。）这些新的特征对处理偏差和方差都有所帮助。理论上，添加更多的特征将增大方差；然而当你发现这种情况时，加入正则化，这可以抵消方差的增加。
减少或者去除正则化（L2 正则化，L1 正则化，dropout）：这将减少可避免偏差，但会增大方差。
修改模型架构（比如神经网络架构）使之更适用于你的问题：这项技术将同时影响偏差和方差。

有一种方法并不能奏效：
添加更多的训练数据：这项技术可以帮助解决方差问题，但它对于偏差通常没有明显的影响。
