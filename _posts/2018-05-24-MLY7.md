---
layout: post
title: ML Yearning（31~35章）
tags: [机器学习, 读书笔记, 总结]
category: 技术
mathjax: true
---
>我的DL启蒙导师Andrew Ng（吴恩达）又出新书了，虽然是草稿，但完全免费（圈起来，要考）也实在太良心了！！！《九阴真经》订阅就送有木有！！。我是第一时间就订阅，每有新的更新就会直接收到邮件。目前收到了前几章，读取来感觉又是太良心了！！！全是工程实践的干货有木有！！对于项目经验欠缺的我来说，真实如获至宝。这个系列的博客我将分享自己读后笔记，欢迎订阅！

## 31.训练集误差分析
除了先前提到的用于处理高偏差的技术外，我通常也会在训练数据上进行误差分析，处理方式类似于在开发集上设置一个 Eyeball 开发集。人为分析几种误差占样本的比例。

## 27.减少方差的技术
* 添加更多的训练数据
* 加入正则化：可降低方差，但却增大了偏差
* early stop: 类似正则化
* 通过特征选择减少输入特征的数量和种类
* 减小模型规模（比如神经元/层的数量）：不推荐，计算成本可接受的话，正则化更好
extra：
* 根据误差分析结果修改输入特征：假设误差分析的结果鼓励你创建额外的特征，从而帮助算法消除某个特定类别的误差。这些新的特征对处理偏差和方差都有所帮助。理论上，添加更多的特征将增大方差；然而当你发现这种情况时，加入正则化，这可以消除方差的增加。
* 修改模型架构（比如神经网络架构）

## 28.诊断偏差与方差：学习曲线
我们通常会有一些“期望错误率”，并希望学习算法最终能够达到该值。例如：
* 如果希望达到人类水平的表现，那么人类错误率可能就是“期望错误率”。
* 如果学习算法为某些产品提供服务，我们可能将主观感受到需什么样的水平才能给用户提供出色的体验。
* 经验

可以根据红色的“开发误差”曲线的走势来推测:
![欠数据](https://blog-img-1257227635.cos.ap-beijing.myqcloud.com/MLY6-1.png)
在上面的例子中，将训练集的大小增加一倍可能会让你达到期望的性能
![达到模型容量](https://blog-img-1257227635.cos.ap-beijing.myqcloud.com/MLY6-2.png)
观察学习曲线可能会帮助你避免花几个月的时间去收集两倍的训练数据，结果却发现这并不管用，比如上面这样。

## 29.绘制训练误差曲线
当数据量变得越来越多时，将很难预测后续红色曲线的走向。因此我们会选择额外的一条曲线来帮助评估添加数据所带来的影响：即训练误差曲线。
![加入蓝线](https://blog-img-1257227635.cos.ap-beijing.myqcloud.com/MLY6-3.png)
训练集只有1,2个时，模型很容易判断正确，随着训练集规模的增加，训练集的误差将越来越大。而算法在训练集上通常比在开发集上做得更好；因此，红色的开发误差曲线通常严格位于蓝色训练错误曲线之上。
 
## 30.解读学习曲线：高偏差
![高可避免方差](https://blog-img-1257227635.cos.ap-beijing.myqcloud.com/MLY6-4.png)
上图的情况可以绝对肯定地说，添加更多的数据并不奏效：
* 蓝线只会不变甚至越来越高
* 红线会在蓝线之上

在训练集大小的最大处（大致对应使用我们的所有训练数据），训练误差和期望性能之间有大的间隙，这代表大的可避免偏差。此外，如果训练曲线和开发曲线之间的间隙小，则表明方差小。
