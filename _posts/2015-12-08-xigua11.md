---
layout: post
title: 西瓜书笔记（六）
tags: [机器学习, 读书笔记, 总结]
category: 技术
mathjax: true
---
>既然开始学习机器学习，那么肯定不能少了经典的西瓜书，作为入门经典教材，讲解的通俗易懂，而且较为系统。工作之余，我也决定读一遍西瓜书，可能大部分内容都有所了解了，这个系列的博客就再总结些自己认为重要的。

# 第11章 特征选择与稀疏学习
针对数据的维度灾难，我们要去掉不相关或是冗余特征。
## 11.1子集搜索与评价
	* 子集搜索：采用贪心算法，在已有的某个子集中，增加或删除一个特征，如果变为更好的子集就保留这步操作。
	* 子集评价：对于给定的数据集D，假定D中第i类样本所占的比例为$p_i$。假设样本属性均为离散型，对于属性子集A，根据其取值将D分成了V个子集$\{D^1,D^2,\ldots,D^V\}$，每个子集在A上取值相同，则A的信息增益为：
	$$Gain(A)=Ent(D)-\sum_{v=1}^V\frac{\left|D^V\right|}{\left|D\right|}Ent(D^v)$$
    其中信息熵的定义为：
    $$Ent(D)=-\sum^{\left|\mathcal{Y}\right|}_{i=1}p_klog_2p_k$$
    将搜索与评价相结合，即可得到特征选择方法。
## 过滤式选择（典型方法Relief）
	先用特征选择对数据进行过滤，之后再用筛过的信息训练模型.著名的*Relief*方法设计了一个*相关统计量*来度量特征的重要性。该统计量是个向量，每一维对应一个初始特征，某个特征子集的重要性由对应的相关统计量分量和确定。Relief为二分类问题设计，变种Relief-F处理多分类的算法如下：
    对于数据$x_i$，若它属于第k类，则先在第k类的样本中寻找$x_i$的最邻近数据$x_{i,nh}$并将其作为猜中近邻，然后在第k类之外的每个类中找到一个$x_i$的最近邻示例作为猜错近邻，记为$x_{i,l,nm}$。于是相关统计量对于属性j的分量为：
    $$\delta^j=\sum_i-\mathtt{diff}(x_i^j,x_{i,nh}^j)^2+\sum_{l{\neq}k}(p_l\times\mathtt{diff}(x_i^j,x_{i,l,nm}^j)^2)$$
    其中$p_l$为第l类样本在数据集D中所占的比例；
    $$\begin{cases}
    \mathtt{diff}(x_a^j,x_b^j)=1,属性j为离散型且x_a^j{\neq}x_b^j\\
    \mathtt{diff}(x_a^j,x_b^j)= \left|x_a^j-x_b^j\right|,else\\
    \end{cases}
    $$
## 包裹式选择（典型方法LVW）
	包裹式特征选择直接把最终的学习器性能作为特征子集的评价标准。效果虽好，但相应的计算开销也要比过滤式大的多。LVW在拉斯维加斯方法（类似蒙特卡洛）下，如果新随机找出的特征子集$A'$使得学习器交叉验证方法的误差减少或是相当，但包含的特征数更少，则代替当前最优子集$A$，需设置终止条件，算法才能停止。
## 嵌入式选择（典型方法L1正则化）
	加入L1正则化，将更容易找到稀疏解，也就是达到降维的目的。
## 稀疏表示（典型方法字典学习）
	数据矩阵具有恰当稀疏表达形式时，对机器学习有很大好处，，使大多数问题变得线性可分，并且降低存储成本。我们想要为普通稠密表达的样本找到合适的字典，转化为稀疏表示：
    $$\min_{B,\alpha_i}\sum_{i=1}^m\left\|x_i-B\alpha_i\right\|_2^2+\lambda\sum_{i=1}^m\left\|\alpha_i\right\|_1$$
    其中$B\in\mathbb{R}^{d{\times}k}$为字典矩阵，k成为字典的词汇量，第一项是希望重构误差尽量小，而第二项则是希望系数更加稀疏。求解上式可以采用变量交替优化的方式。
## 压缩感知
	压缩感知关注的是如何利用信号本身所具有的稀疏性，从部分惯测样本中恢复原信号，分为“感知测量”和“重构恢复”两个阶段，前者就是字典学习，后者是压缩感知的精髓。
    	* RIP限定等距
    对大小为$n{\times}m$，($n<< m$),若$\exists const \delta_k\in(0,1),s.t.\forall\vec{s}$和A的子矩阵$A_K\in\mathbb{R}^{n{\times}k}$:
    $$(1-\delta_k)\left\|s\right\|_2^2\leq\left\|A_ks\right\|_2^2\leq(1+\delta_k)\left\|s\right\|_2^2$$
    则称A满足k限定等距性（k-RIP）,此时可通过下面问题，近乎完美的从y中恢复处稀疏信号，进而恢复出x：
    $$\min_s\left\|s\right\|_0$$
    $$s.t.y=As$$
    	* 矩阵补全
    可用于解决：
    	$$\min_X rank(X)$$
        $$s.t. (X)_{ij}=(A_{ij}),(i,j)\in\Omega$$
        即恢复矩阵的元素应当与观测到的同位置元素相同。
 