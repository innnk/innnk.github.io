---
layout: post
title: 西瓜书笔记（二）
tags: [机器学习, 读书笔记, 总结]
category: 技术
mathjax: true
---
>既然开始学习机器学习，那么肯定不能少了经典的西瓜书，作为入门经典教材，讲解的通俗易懂，而且较为系统。工作之余，我也决定读一遍西瓜书，可能大部分内容都有所了解了，这个系列的博客就再总结些自己认为重要的。

# 第7章 贝叶斯分类器
贝叶斯决策论是概率框架下实施决策的基本方法。
$$ R（c_i|x）=\Sigma_{j=1}^N\lambda_{ij}P(c_j|x)$$
$\lambda$为将真实标记为$c_j$的样本误判称$c_i$所产生的损失。我们的任务就是寻找一个判定准则，最小化总体风险：为最小化总体风险，只需在每个样本上选择那个能使条件风险$R(c|x)$最小的lable。

1. 后验概率
* 给定x,直接建模$P(c|x)$来预测c（决策树、BP神经网络、SVM均属此类）
* 先对联合概率分布$P(x,c)$建模，然后得到$P(c|x)$
 $$ P(c|x)=\frac{p(c)p(x|c)}{p(x)}$$
 其中$P(x|c)$是样本x相对于类标记c的条件概率（似然）
   $P(x)$是用于归一化的”证据“因子（与类标记无关）
   $P(C)$是类”先验“概率（样本空间中各类样本所占的比例）

2.类条件概率的求解（$P(x|c)$）
 * 先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计：假设$P(x|c)$有确定的形式并且被参数向量$\theta_c$唯一确定，则我们可以利用训练集D估计参数$\theta_c$。（在假设样本独立同分布的情况下，可采用极大似然估计，**连乘操作容易造成下溢，通常使用对数似然**）这类方法虽然使类条件概率估计变得简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布。
 * **朴素贝叶斯分类器**：$P(x|c)$是所有属性上的联合概率，难以从有限的训练样本直接估计而得，因此**假设每个属性独立地对分类结果发生影响**那么：
$$P(c|x)=\frac{p(c)p(x|c)}{p(x)}=\frac{p(c)}{p(x)}\prod_{i=1}^dP(x_i|c)$$
**注意：若某个属性值在训练集中没有与某个类同时出现过，则$P(x_i|c)$会为0，那么其他属性将无法影响判断结果。因此需要进行平滑处理：**
$$\hat P(c)=\frac{D_c+1}{D+N}$$
其中N表示训练集D中可能的类别数$\hat P(x_i|c)$类似处理。
* 半朴素贝叶斯分类器
 对属性条件独立性假设进行放宽。例如独依赖（假设每个属性在类别之外最多仅依赖于一个其他属性）
 
3. 贝叶斯网（信念网）

借助有向无环图（DAG）来刻画属性之间的依赖关系，并使用条件概率表（CPT）来描述属性的联合概率分布。为了分析有向图中变量间的条件独立性，可使用“有向分离”，将其转化为无向图（道德化）：
	1.找出有向图中的所有V型结构，在V型结构的链各个父结点之间加上一条无向边
	2.将所有有向边改为无向边
贝叶斯网学习的首要任务就是根据训练数据集来找出结构最“恰当”的贝叶斯网，通常采用“评分搜索”
* 常用的评分函数基于信息论准则，将学习问题看做一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型。但这通常是一个**NP难问题**，因此可以采用：1.贪心算法：从某个网络结构出发，每次调整一条边，直到评分函数不再降低；2.给网络施加约束，例如将结构限定为树。
* **推断:**贝叶斯网训练好之后就可以通过一些属性变量的观测值来推测其他属性变量的取值,但通过联合概率分布来精确计算后验概率（“精确推断”）是NP难问题，因此采取Gibbs采样(随机漫步)完成：
	先生成一个与证据变量E=e一致的样本作为初始点，然后每步从当前样本出发产生下一个样本，算法先假设查询目标值q,有$q^t=q^{t-1}$,然后对非证据变量逐个进行采样改变其取值，采样概率根据贝叶斯网和其他变量的当前取值，假定经过T次采样得到与q一致的样本共有$n_q$个，则可近似估算出后验概率为：
    $$P(Q=q|E=e)\simeq\frac{n_q}{T}$$
    **注意：马尔可夫链需要的稳定时间较长，Gibbs算法收敛较慢，并且若Bayes网中存在极端概率（0or1），则不能保证马尔可夫链存在平稳分布**
* EM算法：当训练样本属性不完整，采用EM算法估计参数隐变量。基本思想是：若模型参数$\Theta$已知，则可根据训练数据推断出最优隐变量Z的值（E步）；反之，若Z的值已知，则可方便地对参数$\Theta$做极大似然估计（M步），反复迭代。