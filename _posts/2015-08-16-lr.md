---
layout: post
title: LR的那些事
tags: [机器学习, 总结]
category: 技术
mathjax: true
---
>总结LR的那些事

替换上面的B1、B2···Bk为特征变量X1，X2，…，Xk，逻辑回归假定支撑A为真而非假的evidence是由于A相关的不同事实或特征变量通过加权线性叠加的方式得来，即

![evidence=\omega_1x_1+\omega_2x_2+...+\omega_kx_k=\omega^Tx](https://www.zhihu.com/equation?tex=evidence%3D%5Comega_1x_1%2B%5Comega_2x_2%2B...%2B%5Comega_kx_k%3D%5Comega%5ETx)

evidence在假设检验里面的定义是一个事件相对其互补事件的对数几率，单位为分贝（db），假设事件A为真的概率为p，那么

![evidence=log\frac{p}{1-p}](https://www.zhihu.com/equation?tex=evidence%3Dlog%5Cfrac%7Bp%7D%7B1-p%7D)

这样一来，可以推出事件A为真的概率为

![h_\omega(x)=g(\omega^Tx)=\frac{1}{1+e^{-\omega^Tx}}](https://www.zhihu.com/equation?tex=h_%5Comega%28x%29%3Dg%28%5Comega%5ETx%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Comega%5ETx%7D%7D)

也就是逻辑回归的模型，其本质是对支撑假设的evidence做线性回归。那么如何求出模型的参数呢？这就是模型学习或训练的问题了。

机器学习的的目标是经验风险最小化（ERM），也就是说先要有个目标才能进行模型的训练，逻辑回归的的学习目标是最大似然，或者是最小化交叉熵损失。 ![J(y,h_\omega(x))= -\frac{1}{m}\sum_{i-1}^{m}[y_ilogh_\omega(x_i) -(1-y_i)log(1-h_\omega(x_i))]](https://www.zhihu.com/equation?tex=J%28y%2Ch_%5Comega%28x%29%29%3D+-%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi-1%7D%5E%7Bm%7D%5By_ilogh_%5Comega%28x_i%29+-%281-y_i%29log%281-h_%5Comega%28x_i%29%29%5D)

对θ求导可得到梯度为：

![G = \frac{1}{m}\sum_{i=1}^mg_i](https://www.zhihu.com/equation?tex=G+%3D+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Emg_i) *，*其中*：* ![g_i = (h_\omega(x_i)-y_i)x_i](https://www.zhihu.com/equation?tex=g_i+%3D+%28h_%5Comega%28x_i%29-y_i%29x_i) 。

## **训练方法**

模型的训练本质上是一个优化的问题，根据场景可分为在线学习与离线学习，当然两者并没有本质的区别，主要在于模型的更新周期不同，一般离线学习的模型可能是日更或周更甚至更长，而在线学习可能是十几分钟跟新一次模型。

## **离线学习**

## **经典算法**

数值优化问题大多基于梯度进行迭代求解，数据量较少时可以使用梯度下降法（GD）:
while not convergence{
![\omega_{t+1} = \omega_t-\eta_tG_t](https://www.zhihu.com/equation?tex=%5Comega_%7Bt%2B1%7D+%3D+%5Comega_t-%5Ceta_tG_t)

}
梯度G的计算是在所有样本上开展的，当数据较多时每一次迭代将花费较多时间与内存，所以一个替代的方案是每次迭代抽取一小部分样本（mini-batch）去估计整个数据集上的梯度，这样可显著加快学习速度。这种算法即为随机梯度下降法（SGD）。
逻辑回归是凸优化问题，SGD在理论上可以收敛到最优解，在实际应用中为了加快收敛速度，前人在SGD的基础上做了改进，产生了adadelta、momentoem、AdaGrad、RMSProp、adm等算法，这些基本上是有supvised learning里面通用的训练算法，在sklearn、tensorflow、spark这类的机器学习框架里面基本上都有实现。

## **OWLQN**

当然工程实际中特征往往非常多，很容易发生过拟合，需要加上正则项，比如常见有L1正则与L2正则，L2正则处处可导，但实际上我们更需要L1正则，因为使用L1正则训练能带来参数的稀疏性，使线上模型更轻，但L1正则项在原点处不可导。OWLQN正是微软针对这个问题提出的算法，它每次迭代时将参数限定在一个象限之内，这样L1正则项变为简单的线性函数，并在原点处使用次梯度（sub-gradient）代替梯度，然后使用L-BFGS更新参数。基本流程如下：

![img](https://pic2.zhimg.com/80/v2-6ec468b07679da452bfd46eac5716d0c_hd.jpg)

与标准L-BFGS法不同的是先计算次梯度

![\diamond_if(x) = \begin{cases} \partial_i^-f(x),\quad if\;\partial_i^-f(x)>0\\ \partial_i^+f(x),\quad if\;\partial_i^+f(x)<0\\ 0 \qquad otherwise. \end{cases}](https://www.zhihu.com/equation?tex=%5Cdiamond_if%28x%29+%3D+%5Cbegin%7Bcases%7D+%5Cpartial_i%5E-f%28x%29%2C%5Cquad+if%5C%3B%5Cpartial_i%5E-f%28x%29%3E0%5C%5C+%5Cpartial_i%5E%2Bf%28x%29%2C%5Cquad+if%5C%3B%5Cpartial_i%5E%2Bf%28x%29%3C0%5C%5C+0+%5Cqquad+otherwise.+%5Cend%7Bcases%7D) 
第2,3步d的一维搜索要求在同一象限内进行，第4步要求估计Hessian矩阵时只需要似然函数的梯度，因为在同一象限内L1-norm为线性函数，其二阶倒数为0.

## **在线学习**

像SGD这种利用mini-batch的数据更新参数的优化方法都可以用于在线学习，目前业界的研究主要围绕着在精度与模型的稀疏性之间的平衡，常见算法有SGD、Tructed Gradient(TG)、FOBOS、Regularized Dual Averaging(RDA)以及FTRL等。

## **基于梯度下降的截断算法**

在线梯度下降算法（OGD）的流程是每次获取一个或一个mini-batch的样本，计算这个样本上损失函数的梯度 ![G_t](https://www.zhihu.com/equation?tex=G_t) *，*然后用下式更新参数

![\omega_{t+1} = \omega_t-\eta_tG_t](https://www.zhihu.com/equation?tex=%5Comega_%7Bt%2B1%7D+%3D+%5Comega_t-%5Ceta_tG_t)

学习率是个非递增的序列，比如 ![1/\sqrt{t}\](https://www.zhihu.com/equation?tex=1%2F%5Csqrt%7Bt%7D%5C)

在求Loss的 梯度时，中L1-norm部分用次梯度，但这并不能有效的给模型的参数带来稀疏性，实际上由于浮点数计算精度的影响，并不会产生绝对为0的参数。一个简单的方案是做每迭代K次做一次截断置0，其中 ![\theta](https://www.zhihu.com/equation?tex=%5Ctheta) 是超参数：

![\omega_{t+1} = \begin{cases} 0, \quad if\quad|\omega_t-\eta_tG_t|\leqslant\theta\\ \omega_t-\eta_tG_t, \quad \;otherwise \end{cases}](https://www.zhihu.com/equation?tex=%5Comega_%7Bt%2B1%7D+%3D+%5Cbegin%7Bcases%7D+0%2C+%5Cquad+if%5Cquad%7C%5Comega_t-%5Ceta_tG_t%7C%5Cleqslant%5Ctheta%5C%5C+%5Comega_t-%5Ceta_tG_t%2C+%5Cquad+%5C%3Botherwise+%5Cend%7Bcases%7D)

当然这种截断的处理方式太过粗暴，某个特征的权重可能是因为缺少样本很少被更新而被截断。TG是在此基础上做了平滑，个人以为治标不治本。FOBOS（L1-FOBOS）可以被看成是TG的一种特殊形式，但截断阈值与学习率（一般非递增）成正比，可以在一定程度上防止由于训练不足过早截断。 这类算法都可以看做是基于OGD（或SGD）的截断算法，基本原理类似。下面介绍一种算法，它基于梯度的平均值更新参数。

## **RDA算法**

RDA也是微软的研究成果，其根据梯度的均值进行截断或更新参数，能消除梯度估计的随机误差带来的影响，但截断阈值固定，各个维度的参数更新方式如下：

	![\omega_{t+1}^i = \begin{cases} 0, \quad if\quad|\overline{g}_i^{(t)}|\leqslant\lambda\\ -\frac{\sqrt{\lambda}}{\gamma}\left[\overline{g}_i^{(t)}-\lambda\,sgn(\overline{g}_i^{(t)})\right], \quad \;otherwise \end{cases}](https://www.zhihu.com/equation?tex=%5Comega_%7Bt%2B1%7D%5Ei+%3D+%5Cbegin%7Bcases%7D+0%2C+%5Cquad+if%5Cquad%7C%5Coverline%7Bg%7D_i%5E%7B%28t%29%7D%7C%5Cleqslant%5Clambda%5C%5C+-%5Cfrac%7B%5Csqrt%7B%5Clambda%7D%7D%7B%5Cgamma%7D%5Cleft%5B%5Coverline%7Bg%7D_i%5E%7B%28t%29%7D-%5Clambda%5C%2Csgn%28%5Coverline%7Bg%7D_i%5E%7B%28t%29%7D%29%5Cright%5D%2C+%5Cquad+%5C%3Botherwise+%5Cend%7Bcases%7D)

FTRL是Google花了好几年的理论研究与工程实验推出的大作，它们对比了前面提到的各种在线学习算法。他们发现L1-FOBOS这类基于梯度下降的方法有较高的精度，但是L1-RDA却能在损失一定精度的情况下产生更好的稀疏性，其区别的关键在于L1-norm的处理方式不同。
FTRL的目标函数为：

![\omega_{t+1}=argmin_\omega(g_{1:t}\cdot\omega+\frac{1}{2}\sum_{s=1}^t{\sigma_s||\omega-\omega_s||_2^2+\lambda_1|\omega|_1}）](https://www.zhihu.com/equation?tex=%5Comega_%7Bt%2B1%7D%3Dargmin_%5Comega%28g_%7B1%3At%7D%5Ccdot%5Comega%2B%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bs%3D1%7D%5Et%7B%5Csigma_s%7C%7C%5Comega-%5Comega_s%7C%7C_2%5E2%2B%5Clambda_1%7C%5Comega%7C_1%7D%EF%BC%89)

这个问题看似更复杂，作者却给出了解析解（这就是大牛与屌丝工程师的差距），如下：

	![\omega_{t+1}^i = \begin{cases} 0, \quad if\;\;|z_i^{(t)}|\leqslant\lambda\\ -\eta_t\left(z_i^{(t)}-\lambda\,sgn(z_i^{(t)})\right) , \quad otherwise \end{cases}](https://www.zhihu.com/equation?tex=%5Comega_%7Bt%2B1%7D%5Ei+%3D+%5Cbegin%7Bcases%7D+0%2C+%5Cquad+if%5C%3B%5C%3B%7Cz_i%5E%7B%28t%29%7D%7C%5Cleqslant%5Clambda%5C%5C+-%5Ceta_t%5Cleft%28z_i%5E%7B%28t%29%7D-%5Clambda%5C%2Csgn%28z_i%5E%7B%28t%29%7D%29%5Cright%29+%2C+%5Cquad+otherwise+%5Cend%7Bcases%7D)

其中 ![z_i^{(t)}=g_{1:t}-\sum_{s=1}^t{\sigma_s}](https://www.zhihu.com/equation?tex=z_i%5E%7B%28t%29%7D%3Dg_%7B1%3At%7D-%5Csum_%7Bs%3D1%7D%5Et%7B%5Csigma_s%7D) ，并且作者采用的学习率为 ![\eta_t=\left(\frac{\beta+\sqrt{n_t}}{\alpha}+\lambda_2\right)^{-1}](https://www.zhihu.com/equation?tex=%5Ceta_t%3D%5Cleft%28%5Cfrac%7B%5Cbeta%2B%5Csqrt%7Bn_t%7D%7D%7B%5Calpha%7D%2B%5Clambda_2%5Cright%29%5E%7B-1%7D) ，而不是SGD中各个维度统一的学习率 ![1/\sqrt{t}](https://www.zhihu.com/equation?tex=1%2F%5Csqrt%7Bt%7D) 。并且每个维度也是单独更新的，大大降低了内存的消耗，具体流程如下：

![img](https://pic4.zhimg.com/80/v2-2d1e45ae11b1cf40a2df036117731e05_hd.jpg)

其实从我个人的理解，产生稀疏性的方式必然是截断，问题的关键是如何有效的截断。某一维度梯度的噪音与样本的稀少都有可能使该维度被“错杀”导致模型精度降低。L1-FOBOS通过动态调整截断阈，可以在一定程度上降低因训练不足而过早截断的风险，L1-RDA利用梯度的均值进行截断，可减少梯度的噪音的影响。而FTRL在某种程度上兼备了两者的优点，一方面每一维使用的是不同的学习速率，如果一个维度特征的训练样本很少，那么该特征维度对应的训练速率可以独自保持比较大的值，降低其被过早截断的风险。另一方面，使用累计梯度进行更新参数与截断判定，也降低了单词梯度计算中噪音的影响。

## **特征工程**

逻辑回归是线性模型，然而在工程实际中，隐藏在原始数据中的最优分类边界往往是非线性的。这时候只用数据的原始特征训练模型效果自然不会理想。这就需要我们构建更加高级的特征，加入到模型中。或者从另一个角度来说，就是把原始数据一一映射到新的空间，使得原始空间中的最优分类边界在新的空间中尽可能地接近线性。从理论上说只要特征工程做得牛逼，无论多么复杂的分类边界，都能以任意误差逼近线性。一个最直观的解就是泰勒展式：
f(x) = f(x0)+f'(x0)(x-x0) + f''(x-x0) 了解泛函的同学应该知道，任意一个函数都可以表示为再生核希尔伯特空间（KHS）中的一个点或向量，这样一来这个函数也就可以表示为 KHS中的基函数的线性组合了。其实这就是核方法的数学基础。当然这里只是为了强调特征工程的重要性，这两种数学理论在这里也可以理解为通过构建特定形式的特征使得原始函数在新的特征空间接近或变成线性函数。
当然，通过这种机械的方式构建特征，效率往往非常低下，产生大量无效特征，且存在共线性（特征相关），而且特征的可解释性也很差。实际构建特征时，要融入我们对所研究的对象的认识。这里以推荐系统与NLP问题为例。

## **推荐系统**

在推荐系统的打分模型（ctr预估）中，数据的原始特征一般分为如下几种：
1.用户（user）特征，如用户的年龄、性别、户籍、职业、星座、偏好、各种画像标签等
2.商品（item）特征，如商品各级类目、价格、各种属性、适用人群、各种标签等
3.上下文（context）特征，如曝光时的时间空间坐标，页面位置，附近其他活动信息，用户近期交互轨迹等。
常见的构建高阶特征的方式是组合与匹配。
先说组合，假如你在做电影推荐，你认为男性更倾向于观看历史、战争与动作片等而女生更更倾向于观看青春偶像片，那么可以将性别（user特征）与剧的类型（item特征）进行组合得到高阶特征gender*x*category。或者你认为很多商品用户倾向于一起购买，比如买啤酒的人可能很有可能买尿不湿，可以将context特征中用户刚加入购物车的商品与召回的商品进行组合，与这样模型就能学习出这些偏好。 
假如你对每个用户购买过的商品类目做了统计，各类占比为，书籍：0.5，服装：0.3，电子产品：0.1，食品：0.1，这个多值特征记为user*c*pref,可将其与商品的类目特征进行匹配，如果商品是书籍，匹配的结果就是0.5，用这样的特征训练的模型，同样一件商品就能根据用户消费习惯的不同，获得不同的优先展示次序。
当然，特征工程师很专业的技术，这也是各个领域的数据科学家的主要技能所在。当然也有自动特征工程，推荐里面常用的如下：
1.DNN：利用大量数据，让机器自己去学习出一种易于分割的分布式特征表示方式（embedding）；

2. GBDT+LR：先训练一个GBDT模型然后用所有子树的叶节点作为LR的特征进行建模。

## **自然语言处理**

如果你现在需要识别互联网新闻中提到的机构名，或者假如你打算为科研单位构建一部科技属于字典，你需要识别paper中的科技术语，这些任务可用LR模型来做，自然语言处理中更多面临的是多分类任务，但特征工程的技术是通用的。
自然语言处理是非常专业的问题，是AI领域单独一个分支，这里只简单提一下，在应用LR或其他经典ML模型时，构建特征的技术。一般拿到一篇文章，在分完句后我们会进行词法分析、句法分析与篇章分析，其中篇章分析目前并不成熟，用得不多，所以主要是利用前两者，生成相应的词汇特征（Lexical Feature）与句法特征（Syntactic feature）。
词法分析的流程有： 词例还原（tokenization）；
词目还原（lemmatization）；
词性标注（POS-tagging）;
词性排歧（POS-Disambiguation）。
句法分析相对复杂，有单纯从结构结构角度分析的如条件上下文无关法，也有从语义角度进行分析的依存关系分析，还有更深层次的分析逻辑含义的一阶谓词演算、语义场等。
在关系抽取中，一个重要的任务是判断一个句子中已经识别出的两个实体之间是否存在我们需要的关系或者对它们之间的关系进行分类。假如我们遇到的句子是：
Astronomer Edwin Hubble was born in Marshfield. Missouri
其对应的POS-tagging与依存句法分析结果如下：![img](https://pic4.zhimg.com/80/v2-d6f2e0155fdf4137d45268011bd890f4_hd.jpg) 

如果用了以下词法特征： 1.两个实体之间的词汇与对应的POS tag序列； 2.两个实体在句子中的先后次序； 3.实体1的左边k个词及对应的POS tag序列； 4.实体2的右边k个词及对应的POS tag序列； 与依存句法特征： 1.两个实体之间的依存关系路径； 2.对于每个实体，各选一个与其有依赖关系的节点，并且这个节点不在依存路径上。 以及各个实体的类型。k分别取{1，2，3}所得结果如下： 

其中POS-tagging可以使用训练好的HMM、CRF、seq2seq、CTC等序列标注模型进行标注，最好用统一领域的标注预料进行训练。依存句法分析可以使用[Dependency parser MINIPAR](https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/6263/00abc08d02a666728bd3da6becad3c379cf5.pdf)进行解析。 

## **连续特征离散化**

有一点值得强调的是，在线性模型中连续特征一般都要做离散化处理，一般用等频切分的方式来处理，当然有业务逻辑更好，比如年龄可以根据童年、少年、青年。。。的逻辑进行切分。这样做的好处是：
1.可以增加模型的非线性度，将连续变量切分成N分，相当于将一个连续特征转换为N个binary的特征，在训练过程中相当于用N阶的阶梯去逼近分类边界而不是一条曲线。
2.提升稳健性，实际线上数据往往有异常数据，比如购买次数1W，用离散化后的特征将异常值归入两端的类别中将不受这种异常值的影响。
3.减少两端误差，自然界中实际的模式往往在两极趋于平缓，用线性模型会有很大误差，离散化后用分段的阶梯建模可以减少这种误差。
4.提升计算速度，所有特征都是binary，而且高度稀疏，会使得训练与在线服务变得高效并且减少能浮点误差。

## **估计误差**

众所周知，在统计推断里面，对一个参数的估计有点估计与区间估计。逻辑回归可以说是给出了点击率的点估计，那么能否给出区间估计呢？或者说，我们你能否知道用这种方式点击率的误差有多大。 逻辑回归是用极大似然估计法（MLE）进行参数估计的。在极大似然估计理论中，有两条重要的推论： 1.如果X^是参数X的极大似然估计量，且Y=f(X)，那么f(X^)即为Y的极大似然估计量。 2.极大似然估计量渐进服从正太分布，且方差为Fisher信息矩阵的逆矩阵，而Fisher信息矩阵的定义是对数似然函数对模型参数的二阶偏倒数的期望值的相反数。 然而通过解析法求出LR模型的估计精度是不现实的，因为一方面经典统计学里面的这些理论只有在你求出似然函数的全局最优解时才成立，另一方面即便求了全局最优解，求解Fisher信息矩阵也不现实，因为参数太多，矩阵元素的数量是特征数量的平方，并且如此大的矩阵很容易呈病态不可逆（可逆也难以计算）。 实际上逻辑回归是用数值优化的方式求解的，整个过程是一个随机动力学过程，最终的结果不仅取决于样本的抽样分布，还取决于初始状态，与迭代的过程，终止条件等因素。在计算统计学中，如此复杂过程的统计推断一般用bootstrap法求解，也就是从训练样本中有放回地抽取一个样本，用相同的超参数设置重新训练一个模型求出待估计的参数，不断重复这样的过程得到一个该参数的样本，即可进行样本估计。 即便如此计算量还是很大，再求简化只能才用启发式的方式进行估计了，也就是为估计误差或不确定性建立一个模型，再用仿真的方式去拟合或验证模型。这就比较难了，你需要对模型的真个学习过程进行分析与简化。例如你认为估计误差主要源于训练过程，而训你过程的动力学方程为$$\omega*{t+1}-\omega*t=\eta\,g\propto x(1-p) \propto x$$ ，那么可近似假设误差正比于\(x\)。

## **线上使用**

在机器学习教材中，一般都假设建模的数据是各态遍历的，或者说与未来可能遇到的所有数据的分布是一致的。然而在工程实际中，这种假设几乎不可能成立，建模所用的数据的分布与生产环境的数据的分布都有无法避免的差异（通过提高抽样技术与扩大训练样本在一定程度上可以减小这种差异）。此外建模过程的一些不合理假设以及部分特征在训练或线上打分时无法提供也会使模型产生系统偏差。所以为了进一步提升，往往在数据的不同区域d分别对逻辑回归打分的结果p再加上一个纠正项τd，常用的模型是τ(p) = γpκ，参数一般在各自区域d用保序回归（isotonic regression）进行学习，这不仅增加模型的非线性度，而且减小了两极的误差。

 

逻辑回归算法的代价函数如下：

![J=1/m*\sum_{i=1}^{m}({h(x_i)-y_i} )^2](https://www.zhihu.com/equation?tex=J%3D1%2Fm%2A%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%28%7Bh%28x_i%29-y_i%7D+%29%5E2)（m是y总数，嫌麻烦的话，和第二周一样，如果你把那个加和符号去掉，那么这里x和y就变成向量了也就是：![J=1/m*({h(x)-y} ).^2](https://www.zhihu.com/equation?tex=J%3D1%2Fm%2A%28%7Bh%28x%29-y%7D+%29.%5E2)。）

然后因为y只有0和1两个取值，这里就可以分两种情况讨论，让y从函数中去掉：

y=0：![J=1/m*\sum_{1}^{m}({h(x)-0} )^2](https://www.zhihu.com/equation?tex=J%3D1%2Fm%2A%5Csum_%7B1%7D%5E%7Bm%7D%28%7Bh%28x%29-0%7D+%29%5E2)

y=1:![J=1/m*\sum_{1}^{m}({h(x)-1} )^2](https://www.zhihu.com/equation?tex=J%3D1%2Fm%2A%5Csum_%7B1%7D%5E%7Bm%7D%28%7Bh%28x%29-1%7D+%29%5E2)

然而我们发现，这样拿出来的函数，因为y一会儿是1，一会儿是0，得到的图像如下（左边的）然而左边的凹凸不平（非凸），不方便梯度下降，我们想要的是右边那种滑溜溜（凸）的图像。：

![img](https://pic2.zhimg.com/80/7ce98c3ddfb66f85017fc4af7a423176_hd.jpg)

 

所以我们假设有个函数cost，

 

原函数J变成了![J=1/m*\sum_{1}^{m}(cost(h(x),y)](https://www.zhihu.com/equation?tex=J%3D1%2Fm%2A%5Csum_%7B1%7D%5E%7Bm%7D%28cost%28h%28x%29%2Cy%29)

反正我们要的是![cost=(h(x),y)](https://www.zhihu.com/equation?tex=cost%3D%28h%28x%29%2Cy%29)符合决策边界的条件就可以了，反正都把结果四舍五入成1或者0了，也没必要非得是某个函数对吧？

所以它是什么函数无所谓。只要（它的函数处理了x以后，得到的y）符合决策边界的条件即可。而现在我们希望得到一个滑溜溜的凸函数。那么我们就假设（注意，这是假设出来的，符合条件的函数之一，没说非得这样。然而这是已有的函数中最好的。）：

y=1的时候，![cost=(h(x),y)=-log(h(x))](https://www.zhihu.com/equation?tex=cost%3D%28h%28x%29%2Cy%29%3D-log%28h%28x%29%29)

y=0的时候，![cost=(h(x),y)=-log(1-h(x))](https://www.zhihu.com/equation?tex=cost%3D%28h%28x%29%2Cy%29%3D-log%281-h%28x%29%29)

它们的图像如下：

![img](https://pic1.zhimg.com/80/49c392586f8b1280d37be6e0cc62e00e_hd.jpg)

符合条件。那么这时候还有一个问题，大家都很懒对吧，懒得分情况讨论，那么如何做呢？

你看，由于y只有1和0两个值，所以我们就直接把两种情况的cost函数相加，然后分别乘以y和y-1即可：

 

看，是不是很方便呢？

于是原来的那个代价函数J就变成了：

 

然后照旧，我们目的是拿x和y训练样本得到![\theta](https://www.zhihu.com/equation?tex=%5Ctheta)嘛！所以对这玩意求导，然后取个α，然后不断重复梯度下降……以得到![\theta](https://www.zhihu.com/equation?tex=%5Ctheta)，就是下面这个玩意：

（repeat：）![\theta_j=\theta_j-\alpha *\frac{d}{d\theta_j}J(\theta) ](https://www.zhihu.com/equation?tex=%5Ctheta_j%3D%5Ctheta_j-%5Calpha+%2A%5Cfrac%7Bd%7D%7Bd%5Ctheta_j%7DJ%28%5Ctheta%29+)

这坨玩意里，我们需要计算![\frac{d}{d\theta_j}J(\theta) ](https://www.zhihu.com/equation?tex=%5Cfrac%7Bd%7D%7Bd%5Ctheta_j%7DJ%28%5Ctheta%29+)，直接照搬之前笔记里的，![\frac{d}{d\theta_j}J(\theta) =1/m*\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}](https://www.zhihu.com/equation?tex=%5Cfrac%7Bd%7D%7Bd%5Ctheta_j%7DJ%28%5Ctheta%29+%3D1%2Fm%2A%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7B%28h%28x_i%29-y_i%29%7D+%2Ax_%7Bi%2Cj%7D)即可。只不过这里的![h(x)=\frac{1}{1+e^{-\theta^T*X} } ](https://www.zhihu.com/equation?tex=h%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ET%2AX%7D+%7D+)

于是我们最终得到的

（repeat：）![\theta_j=\theta_j-\alpha /m*\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}](https://www.zhihu.com/equation?tex=%5Ctheta_j%3D%5Ctheta_j-%5Calpha+%2Fm%2A%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7B%28h%28x_i%29-y_i%29%7D+%2Ax_%7Bi%2Cj%7D)

逻辑回归函数就是这样了。

然后是拟合问题，下面是3种情况：

![img](https://pic4.zhimg.com/80/c278dcb3c42ff6b9c7d914b85030d77d_hd.jpg)

第一种是欠拟合，通常是因为特征量选少了。第二种是我们想要的，第三个是过拟合，通常是因为特征量选多了。

欠拟合的解决方法是增加特征量。

过拟合的解决方法是减少特征量或者正则化。

比如我们的逻辑回归函数，不选个自定义的函数，就用我们那个类似泰勒展开式的函数来做的画，这货长得凹凸不平的

![img](https://pic4.zhimg.com/80/e77581efac0120995f21720c424c0741_hd.jpg)

，一点都不光滑。那么，按照这货拟合回来的函数，十有八九也是过拟合了。于是我们就会得到一个类似这样的决策边界（蓝色线）显然，这条决策边界很……不实用。

![img](https://pic2.zhimg.com/80/b1d00724220a2791374343761ccf4b3f_hd.jpg)

我们想要的，是一根滑溜溜的凸函数，但是我们又不能确定哪些特征量该去掉，所以我们就选择正则化的方式解决过拟合。

正则化的方法，就是给代价函数后面加个“惩罚项”……来降低它对数据的拟合能力。

于是我们的![J=1/2m*\sum_{1}^{m}(-y*log(h(x))+(y-1)log(1-h(x)))](https://www.zhihu.com/equation?tex=J%3D1%2F2m%2A%5Csum_%7B1%7D%5E%7Bm%7D%28-y%2Alog%28h%28x%29%29%2B%28y-1%29log%281-h%28x%29%29%29)

就变成了：![J=1/2m*(\sum_{1}^{m}(-y*log(h(x))+(y-1)log(1-h(x)))+\lambda \sum_{j=1}^{n}{\theta_j^2} )](https://www.zhihu.com/equation?tex=J%3D1%2F2m%2A%28%5Csum_%7B1%7D%5E%7Bm%7D%28-y%2Alog%28h%28x%29%29%2B%28y-1%29log%281-h%28x%29%29%29%2B%5Clambda+%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%7B%5Ctheta_j%5E2%7D+%29)（这里n表示特征量的总数，意思就是让所有的n个正义的![\theta](https://www.zhihu.com/equation?tex=%5Ctheta)为了解决过拟合问题，大喊一声“合体！”然后一起来惩罚那个过度拟合了的函数……![\lambda ](https://www.zhihu.com/equation?tex=%5Clambda+)是正规化参数，决定了你惩罚得有多狠。你要惩罚狠点，你就把![\lambda ](https://www.zhihu.com/equation?tex=%5Clambda+)提高一点，![\lambda ](https://www.zhihu.com/equation?tex=%5Clambda+)过高会变得欠拟合，![\lambda ](https://www.zhihu.com/equation?tex=%5Clambda+)过小无法解决过拟合。）

那么我们的![\theta_j=\theta_j-\alpha /m*\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}](https://www.zhihu.com/equation?tex=%5Ctheta_j%3D%5Ctheta_j-%5Calpha+%2Fm%2A%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7B%28h%28x_i%29-y_i%29%7D+%2Ax_%7Bi%2Cj%7D)就顺利变成了：

![\theta_j=\theta_j(1-\alpha *\lambda /m)-\alpha /m*\sum_{i=1}^{m}({(h(x_i)-y_i)} *x_{i,j})](https://www.zhihu.com/equation?tex=%5Ctheta_j%3D%5Ctheta_j%281-%5Calpha+%2A%5Clambda+%2Fm%29-%5Calpha+%2Fm%2A%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%28%7B%28h%28x_i%29-y_i%29%7D+%2Ax_%7Bi%2Cj%7D%29) ~→

![\theta_j=\theta_j-\alpha /m*(\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}+\lambda /m*\theta_j)](https://www.zhihu.com/equation?tex=%5Ctheta_j%3D%5Ctheta_j-%5Calpha+%2Fm%2A%28%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7B%28h%28x_i%29-y_i%29%7D+%2Ax_%7Bi%2Cj%7D%2B%5Clambda+%2Fm%2A%5Ctheta_j%29)。

注意，当![\theta_0](https://www.zhihu.com/equation?tex=%5Ctheta_0)的时候，由于![x_0](https://www.zhihu.com/equation?tex=x_0)=1，所以这一项不会欠拟合也不会过拟合，所以不惩罚它。

然后，如果你用的不是线性回归，而是正规方程的话，同理，给加个惩罚项就好了。这里加的惩罚项为![\varsigma ](https://www.zhihu.com/equation?tex=%5Cvarsigma+),![\varsigma ](https://www.zhihu.com/equation?tex=%5Cvarsigma+) 是一个n+1阶的单位矩阵把第一项变成0（因为第一项不惩罚），我们把写作varsigma