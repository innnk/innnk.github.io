---
layout: post
title: 西瓜书笔记（五）
tags: [机器学习, 读书笔记, 总结]
category: 技术
mathjax: true
---
>既然开始学习机器学习，那么肯定不能少了经典的西瓜书，作为入门经典教材，讲解的通俗易懂，而且较为系统。工作之余，我也决定读一遍西瓜书，可能大部分内容都有所了解了，这个系列的博客就再总结些自己认为重要的。

# 第10章 降维与度量学习 
## 10.1 k临近学习（kNN）
* 简单的例子

* 在**密采样**假设下：任意测试样本x附近任意小的δ范围内，总能找到一个训练样本。

* k值较小，预测结果对邻近实例点非常敏感，容易发生过拟合。在应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优k值
* 为提高k邻近搜索的效率，可使用kd树方法（选取中位数作为切分点）
	
	*泛化错误率不超过贝叶斯最优分类器的错误率的两倍。*
## 10.2 低维嵌入
1. 一些概念

	* “维数灾难”：密采样假设实际很难实现，在高维情形下出现的数据样本系数、距离计算困难。
	* “维数约减”：降维前后各样本距离尽可能接近。
2. 一些算法
	* 多维缩放(Multiple Dimensional Scaling)	
			
			1.先求出矩阵A各行、各列以及各元素的平方均值；
			2.根据距离不变的准则，构造一个同维度的矩阵B；
			3.然后求B矩阵的前d'(d'<<rank(B))个特征值，进行构造。

	* 线性降维：乘以正交变换矩阵（d'个d维基向量）。
## 10.3 主成分分析（PCA）
* 构建一个超平面，st.样本到这个超平面度的距离足够近<==>各样本在该超平面上的投影尽量分散。
* 算法：
		
		1.计算样本的协方差矩阵，并进行特征值分解；
		2.取d'个最大的特征值所对应的特征值向量w1,w2...,wd';
		3.投影矩阵W=（w1,w2,...,wd'）。
## 10.4核化线性降维
通过非线性映射$\phi$，找到合适的低维嵌入，我们称$\kappa(x_i,x_j)=\phi{(x_i)^T}\phi(x_j)$为核函数。
## 10.5流形学习
1. 概念

	“流形”是局部与欧式空间同胚的空间，对于低维流形嵌入到高维空间的情况，由于其局部还是满足欧式空间的性质。
2. 等度量映射（保持临近样本距离不变）
	
	高维空间中的直线，在低维流形中是不可达的，因此在低维流形中测量样本之间的距离，要计算“测地线”距离，采用Dijkstra算法，计算样本间距离。
3. 局部性嵌入（保持临近样本之间的线性关系不变）
	
	先为每个样本$x_i$找到其临近下标集合$Q_i$,然后计算出基于$Q_i$中的样本点对$x_i$进行线性重构的系数$\omega_i$:
$$\min_{\omega_1,\omega_2,\ldots,\omega_m}=\sum_{i=1}^m\left\|x_i-\sum_{j{\in}Q_i}\omega_{ij}x_j\right\|_2^2$$
$$s.t. \sum_{j{\in}Q_i}^m\omega_{ij}=1$$
## 10.6度量学习
降维的主要目的就是寻找**更优的距离度量方法**，因此可以直接想法找到一个在原空间更优的度量方法。对于两个d维样本$dist_{ij,k}$表示$x_i$和$x_j$在第k维（某个属性）的距离，若各个属性重要度不同，我们赋予不同的权值$\omega_k$,即：
$$dist_{wed}^2(x_i,x_j)=\sum_{k=1}^k\omega_i{\cdot}dist_{ij,k}^2=(x_i-x_j)^TW(x_i-x_j)$$
但W是*对角阵*，意味着各维度正交，即各属性之间没有关系，不符合实际情况，因此我们将W改写为正定矩阵M（度量矩阵），此时得到的距离成为*“马氏距离”*，然后我们通过设置目标，对M进行学习。