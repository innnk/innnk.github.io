---
layout: post
title: 深度学习深度学习
tags: [deep learning, 总结]
category: 技术
mathjax: true
---
## lr遇到缺失值的问题

选择实数 0 来替换所有缺失值，恰好能适用于Logistic 回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值。
$$\omega = \omega + \alpha * x_i $$
如果 dataMatrix 的某个特征对应值为 0，那么该特征的系数将不做更新；另外，由于 Sigmoid(0) = 0.5 ，即它对结果的预测不具有任何倾向性，因此我们上述做法也不会对误差造成任何影响。基于上述原因，将缺失值用 0 代替既可以保留现有数据，也不需要对优化算法进行修改。此外，该数据集中的特征取值一般不为 0，因此在某种意义上说它也满足 “特殊值” 这个要求。

## 生成模型和判别模型
 —|生成模型|判别模型
---|---|---
求解目标|先求出联合概率P（X,Y），然后求出条件概率分布P（Y&#124;X）|直接求解P（Y&#124;X）
特点|学习收敛速度更快；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用|直接面对预测，往往学习的准备率更高；由于直接学习P（Y&#124;X）或者f(x)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题
典型模型|朴素贝叶斯、隐马尔可夫|knn,nn,tree-base,lr,svm,boosting，条件随机场

## 感知机学习算法的对偶形式
1. 标准形式
    感知机模型为$f(x) = sign({\omega}·x+b)$
* 选取初值${\omega}_0,b_0$
* 在训练集中选取数据$(x_i,y_i)$
* 如果$y_i({\omega}·x_i+b)≤0$
$$\omega\leftarrow\omega+{\eta}y_ix_i$$

$$b{\leftarrow}b+{\eta}y_i$$

* 不断迭代，直到分类全部正确

2. 对偶形式
    根据上面不难看出，若假设参数的初始值均为0，最后学习到的参数可以分别表示为：
    $$\omega = \sum_{i=1}^N\alpha_iy_ix_i$$
    $$b = \sum_{i=1}^N\alpha_iy_i$$
    其中$\alpha_i =n_i\eta$(n表示修改了几次)
    感知机模型$f(x)=sign(\sum_{j=1}^N\alpha_jy_jx_j·x+b)$
* $\alpha, b$设为0
* 在训练集中选取数据$(x_i,y_i)$
* 如果$y_i(\sum_{j=1}^N\alpha_jy_jx_j·x+b)≤0$
  $$\alpha_i \leftarrow\alpha_i+\eta $$
  $$b \leftarrow b+\eta y_i $$
  对偶形式中训练示例仅以内积的形式出现。为了方便可以将训练集中实例间的内积计算出来并以矩阵形式存储（Gram矩阵）
## 基于梯度的学习

1. 用于二值输出分布的sigmoid单元

  $$J(\theta) = -{\log}P(y|x) = -{\log}\sigma((2y-1)z)= \zeta((1-2y)z)$$
 * 因为用于最大似然的代价函数是 $−{\log}P(y | x)$，代价函数中的 `log `抵消了 sigmoid 中的 `exp`。如果没有这个效果，sigmoid 的饱和性会阻止基于梯度的学习做出好的改进。通过将损失函数写成 softplus 函数的形式，我们可以看到它仅仅在$(1 − 2y)z$取绝对值非常大的负值时才会饱和。因此饱和只会出现在模型已经得到正确答案时,最大似然几乎总是训练 sigmoid 输出单元的优选方法。
 * 缺点：容易出现梯度消失，函数输出并不是zero-centered（会使训练速度下降），幂运算相对耗时
     关于非zero-centered的输出会使训练下降的原因：当考虑线性加权和sigmoid激活，当x均大于0 时，gradient更新永远大于0（x小于0的情况类似），假设我们在更新gradient的理想过程中，是希望w1不断增加，w2不断减少，当输入x永远大于0或者小于0时收敛会很慢（zig-zagging dynamics），但如果我们使用mini-batch的方法，每次x有所不同，有正有负，就会加速这个进程，可以变向解决这个问题
 * `tanh`解决了zero-centered输出的问题，但其他两点仍然存在
2. 用于多值输出分布的softmax单元
    $${\log}softmax(z)_i=z_i - {\log}\sum_j\exp(z_j)$$

  * 式中的第一项表示输入$z_i$总是对代价函数有直接的贡献。因为这一项不会饱和，所以即使$z_i$对式的第二项的贡献很小，学习依然可以进行。当最大化对数似然时，第一项鼓励$z_i$被推高，而第二项则鼓励所有的z被压低。为了对第二项${\log}\sum_j\exp(z_j)$ 有一个直观的理解，注意到这一项可以大致近似为 $\max_jz_j$ 。这种近似是基于对任何明显小于$\max_jz_j$的$z_i$,$\exp(z_k)$都是不重要的。负对数似然代价函数总是强烈地惩罚最活跃的不正确预测。如果正确答案已经具有了softmax的最大输入，那么$-z_i$ 项和${\log}\sum_j\exp(z_j)$项将大致抵消。
  * 除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习。特别是，**平方误差**对于 softmax 单元来说是一个很差的损失函数.
  * 对于 softmax 的情况，它有多个输出值。当输入值之间的差异变得极端时，这些输出值可能饱和。因此进行如下转化：

  $$softmax(z)=softmax(z-\max_iz_i)$$

3. 隐藏层输出函数Relu
  * 更容易学习优化，因为其分段线性性质，导致其前传、后传，求导都是分段线性。而sigmoid，由于两端饱和，在传播中容易丢弃信息
  * 缺点：是不能用Gradient-Base方法，同时如果deactive,容易无法再次active(可采用maxout激活函数解决，或者采用Xavier初始化方法，并且避免learning rate过大，在或者使用PRelu等负数部分不为0的优化版)，输出不是zero-centered。
  * 神经网络训练的软件实现通常返回左导数或右导数的其中一个，而不是报告导数未定义或产生一个错误。在实践中，我们可以放心地忽略隐藏单元激活函数的不可微性。
4. 隐藏层输出函数maxout
    因为每个单元由多个过滤器驱动，maxout单元具有一些冗余来帮助它们抵抗一种被称为灾难遗忘的现象

## 参数范数惩罚
1. L2范数
    $${\omega}\leftarrow{\omega}-{\epsilon}(\alpha{\omega}+\nabla_{\omega}J({\omega};X,y))$$
    我们可以看到，加入权重衰减后会引起学习规则的修改，即在每步执行通常的梯度更新之前先收缩权重向量（将权重向量乘以一个常数因子）。只有在显著减小目标函数方向上的参数会保留得相对完好。在无助于目标函
    数减小的方向（对应Hessian矩阵较小的特征值）上改变参数不会显著增加梯度。这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。

2. L1范数
    $$\nabla_{\omega}{\hat J}({\omega};X,y))={\alpha}sign(\omega)+\nabla_{\omega}J({\omega};X,y))$$
   * 正则化对梯度的影响不再是线性地缩放每个$\omega_i$而是添加了一项与$sign(\omega_i)$同号的常数。使用这种形式的梯度之后，我们不一定能得到 $J({\omega};X,y)$ 二次近似的直接算术解（L 2 正则化时可以）。
   * 相比L2正则化,L1正则化会产生更稀疏的解。此处稀疏性指的是最优值中的一些参数为0。和L2正则化相比,L1正则化的稀疏性具有本质的不同。由 L 1 正则化导出的稀疏性质已经被广泛地用于特征选择机制。特征选择从可用的特征子集选择出有意义的特征，化简机器学习问题。
   * LASSON(L1+最小二乘loss function)
   * 许多正则化策略可以被解释为MAP贝叶斯推断，特别是L2正则化相当于权重是高斯先验的 MAP 贝叶斯推断。对于 L 1 正则化，用于正则化代价函数的惩罚项 $αΩ(w) = α\sum_i |w_i |$与通过MAP贝叶斯推断最大化的对数先验项是等价的（$w ∈ R_n$ 并且权重先验是各向同性的拉普拉斯分布.
   * 相较于L2，其有稀疏性：两种 regularization 能不能把最优的 x 变成 0，取决于原先的费用函数在 0 点处的导数。如果本来导数不为 0，那么施加 L2 regularization 后导数依然不为 0，最优的 x 也不会变成 0。而施加 L1 regularization 时，只要 regularization 项的系数 C 大于原先费用函数在 0 点处的导数的绝对值，x = 0 就会变成一个极小值点。l1对于小值的惩罚，比l2的大。
      ![l1](http://ovapaz2zv.bkt.clouddn.com/l1.png)
3. early stopping
    提前终止是一种正则化策略，不仅可以通过展示验证集误差的学习曲线是一个 U 型曲线来支持这种说法；还可以认为提前终止可以将优化过程的参数空间限制在初始参数值$θ_0$的小邻域内。想象用学习率 ϵ 进行 τ 个优化步骤，ϵτ 的效果就好像是权重衰减系数的倒数。

4. 参数共享和绑定
    我们经常想要表达的一种常见依赖是某些参数应当彼此接近。参数范数惩罚是正则化参数使其彼此接近的一种方式，而更流行的方法是使用约束：强迫某些参数相等。对于某些特定模型，如卷积神经网络，这可能可以显著减少模型所占用的内存。

5. 稀疏表示
    权重衰减直接惩罚模型参数。另一种策略是惩罚神经网络中的激活单元，稀疏化激活单元。输入的表示越稀疏，惩罚项越小。
## 数据增强

* 对于图像可以翻转，平移，截取等等
* 在NN的输入层注入噪声，对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚 。在一般情况下，注入噪声远比简单地收缩参数强大
* 隐藏层添加噪声：特别是噪声被添加到隐藏单元时会更加强大。另一种正则化模型的噪声使用方式是将其加到权重。这项技术主要用于循环神经网络。
* 输出层添加噪声：大多数数据集的 y 标签都有一定错误。错误的 y 不利于最大化 ${\log}P(y | x)$。避免这种情况的一种方法是显式地对标签上的噪声进行建模。标签平滑（label smoothing）通过把确切分类目标从 0 和1 替换成$\fracϵ{k−1}$和$1−ϵ$，正则化具有 k 个输出的softmax函数的模型。标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。

## 集成方法
1. Bagging
    Bagging是通过结合几个模型降低泛化误差的技术。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。它通过在同一个训练集上重复采样，训练出不同的模型，最后取平均，由于子样本集的相似性以及使用的是相同模型，因此各模型有近似相等的bias和variance。这是由于:

    ![E](https://blog-img-1257227635.cos.ap-beijing.myqcloud.com/E.png )

    而若各子模型独立，有:

    ![v](https://blog-img-1257227635.cos.ap-beijing.myqcloud.com/V.png )

2. Random Forest
    为了进一步降低Variance,RF通过随机选取变量子集做拟合的方式decorrelated(降低相关性)了各个子模型，使Variance进一步降低：
* 设有i.d.的n个随机变量，方差为$\sigma^2$,两两变量之间相关性记为$\rho$，则有：
  ![v](https://blog-img-1257227635.cos.ap-beijing.myqcloud.com/ve.png )
  bagging只降低第二项，而RF同时降低两项。

3. boosting
    使用forward-stagewise（在迭代的第n步，求解新的子模型f(x)及组合系数a，来最小化$L(y,f_{n-1}(x)+af_n(x))$这里$f_{n-1}(x)$是前n-1步得到的子模型的和）这种贪心法去最小化loss,因此boosting是序列化的最小化loss,其bias自然逐步下降，当由于这种sequential及adaptive的策略，各子模型之间是强相关的，所以模型之和不能显著降低Variance。

## 模型优化
1. 小批量大小通常由以下因素决定：

  * 更大的批量会计算更精确的梯度估计，但是回报却是小于线性的。
  * 极小批量通常难以充分利用多核架构。这促使我们使用一些绝对最小批量，低于这个值的小批量处理不会减少计算时间。
  * 如果批量处理中的所有样本可以并行地处理（通常确是如此），那么内存消耗和批量大小会正比。对于很多硬件设施，这是批量大小的限制因素。
  * 在某些硬件上使用特定大小的数组时，运行时间会更少。尤其是在使用GPU时，通常使用 2 的幂数作为批量大小可以获得更少的运行时间。一般，2 的幂数的取值范围是 32 到 256，16 有时在尝试大模型时使用。
  * 可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化效果。泛化误差通常在批量大小为 1 时最好。因为梯度估计的高方差，小批量训练需要较小的学习率以保持稳定性。因为降低的学习率和消耗更多步骤来遍历整个训练集都会产生更多的步骤，所以会导致总的运行时间非常大。

2. Hessian矩阵病态
    病态体现在随机梯度下降会"卡" 在某些情况，此时即使很小的更新步长也会增加代价函数。判断病态是否不利于神经网络训练任务，我们可以监测平方梯度范数$g^⊤g$ 和$g^⊤Hg$。在很多情况中，梯度范数不会在训练过程中显著缩小，但是 g ⊤ Hg 的增长会超过一个数量级。其结果是尽管梯度很强，学习会变得非常缓慢，因为学习率必须收缩以弥补更强的曲率。

3. 参数初始化
  * 关于如何初始化网络，正则化和优化有着非常不同的观点。优化观点建议权重应该足够大以成功传播信息，但是正则化希望其小一点。诸如随机梯度下降这类对权重较小的增量更新，趋于停止在更靠近初始参数的区域（不管是由于卡在低梯度的区域，还是由于触发了基于过拟合 的提前终止准则）的优化算法倾向于最终参数应接近于初始参数。我们可以将初始化参数$θ$为$θ_0$类比于强置均值为$θ_0$的高斯先验$p(θ)$。从这个角度来看，选择$θ_0$接近0是有道理的。这个先验表明，单元间彼此互不交互比交互更有可能。只有在目标函数的似然项表达出对交互很强的偏好时，单元才会交互。另一方面，如果我们初始化$θ_0$为很大的值，那么我们的先验指定了哪些单元应互相交互，以及它们应如何交互。
  * 设置偏置的方法必须和设置权重的方法协调。设置偏置为零通常在大多数权重初始化方案中是可行的。存在一些我们可能设置偏置为非零值的情况：
     1.如果偏置是作为输出单元，那么初始化偏置以获取正确的输出边缘统计通常是有利的；
     	2.我们可能想要选择偏置以避免初始化引起太大饱和。例如，我们可能会将 ReLU 的隐藏单元设为 0.1 而非 0，以避免 ReLU 在初始化时饱和。
     	3.有时，一个单元会控制其他单元能否参与到等式中，在这种情形下，我们希望设置偏置 h，使得在初始化的大多数情况下 h ≈ 1。否则，u 没有机会学习。

4. Batch Normalization
    BN解决的是Inter Covariate Shift问题，即由于每一层的参数都在不断变化，所以输出的分布也会不断变化，造成梯度需要不断适应新的数据分布，并且如果每一层的scale不一致，实际上每层需要的学习率也是不一样的，同一层不同维度的scale往往不同，通常需要使用最小的那个学习率才能保证loss有效下降，所以在每一个minibatch里，对每个维度进行归一化，同时为了增加BN的表现力（不至于都压缩在标准高斯空间内），再加上了两个参数（期望和方差的开方）

## 卷积网络

1. 三个重要思想
  * 稀疏交互：这是使核的大小远小于输入的大小来达到的
  * 参数共享：在一个模型的多个函数中使用相同的参数
  * 等变表示: 如果一个函数满足输入改变，输出也以同样的方式改变这一性质。e.g：函数把I中的每个像素向右移动一个单位。如果我们先对I进行这种变换然后进行卷积操作所得到的结果，与先对I进行卷积然后再对输出使用平移函数 g 得到的结果是一样的
2. 无限强的先验
    一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对于这些参数的值给出了多大的支持。我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。类似的，使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。**这将导致卷积和池化可能导致欠拟合。与任何其他先验类似，卷积和池化只有当先验的假设合理且正确时才有用。如果一项任务依赖于保存精确的空间信息，那么在所有的特征上使用池化将会增大训练误差。**
3. 关键层
    * 输入层，对数据去均值，做data augmentation等工作
    * 卷积层，局部关联抽取feature
    * 激活层，非线性变化
    * 池化层，下采样，局部不变性（如图像翻转，倾斜等）
    * 全连接层，增加模型非线性
    * 高速通道，快速连接
    * BN层，缓解梯度弥散

## 手动调参

| 超参数       | 容量何时增加 | 原因                                                         | 注意事项                                                     |
| ------------ | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 隐藏单元数量 | 增加         | 增加隐藏单元数量会增加模型的表示能力                         | 几乎模型每个操作所需的时间和内存代价都会随隐藏单元数量的增加而增加。 |
| 学习率       | 调至最优     | 不正确的学习速率，不管是太高还是太低都会由于优化失败而导致低有效容量的模型。 |                                                              |
| 卷积核宽度   | 增加         | 增加卷积核宽度会增加模型的参数数量。                         | 较宽的卷积核导致较窄的输出尺寸，除非使用隐式零填充减少此影响，否则会降低模型容量。较宽的卷积核需要更多的内存存储参数，并会增加运行时间，但较窄的输出会降低内存代价。 |
| 隐式零填充   | 增加         | 在卷积之前隐式添加零能保持较大尺寸的表示。                   | 大多数操作的时间和内存代价会增加。                           |
| 权重衰减系数 | 降低         | 降低权重衰减系数使得模型参数可以自由地变大。                 |                                                              |
| Dropout比率  | 降低         | 较少地丢弃单元可以更多地让单元彼此“协力”来适应训练集。       |                                                              |

# 权重初始化
* 全部初始化为相同值，容易输出及更新完全相同，各个节点没有区别，降低模型表达能力;
* 标准均匀初始化方法:
    ```
    1.保证了激活函数的输入值的均值为0，方差为常量1/3;
    ```
    | 方法              | 表达式                                                     | 特点                                                         | 适用                                                      |
    | ----------------- | ---------------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------- |
    | 标准正态初始化    | $Wi,j∼N(0,\frac{1}{\sqrt{n_{in}}})$                        | 输入值的均值为0，方差为1，和网络的层数和神经元的数量无关     | 输出均值为0的激活函数(`tanh`)                             |
    | 标准均匀初始化    | $Wi,j∼U(-\frac{1}{\sqrt{n_{in}}},\frac{1}{\sqrt{n_{in}}})$ | 输入值的均值为0，方差为常量1/3，和网络的层数和神经元的数量无关 | 输出均值为0的激活函数(`tanh`)                             |
    | Xavier初始化      | $Wi,j∼N(0,\sqrt\frac{2}{n_{in}+n_{out}})$                  | 各层的激活值和状态梯度的方差在传播过程中的方差保持一致       | 激活函数在零点附近接近线性函数，且激活值关于0对称(`tanh`) |
    | He初始化          | $Wi,j∼N(0,\sqrt\frac{2}{{n_{in}}})$                        | 假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2 | 适用于ReLU                                                |
    | SVD（正交）初始化 | 将W初始化为正交矩阵                                        | 由于正交矩阵的特征值的绝对值为1，矩阵连乘不会梯度爆炸        | RNN                                                       |
* Xavier详解：
   ![xavier](http://ovapaz2zv.bkt.clouddn.com/xavier.png)
# 特殊层的反向传播
* Relu在0点处导数为1

* pool-mean
  ![img](http://ovapaz2zv.bkt.clouddn.com/pool-mean.jpg)
* pool-max
  ![img](http://ovapaz2zv.bkt.clouddn.com/pool-max.jpg)

# PCA-whitening就是在PCA的基础上再归一化方差。
# 训练技巧
* 要做梯度归一化,即算出来的梯度除以minibatch size

* clip c(梯度裁剪): 限制最大梯度,其实是`$value = \sqrt{(w1^2+w2^2….)}$`,如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15



* dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置.关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329



* adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好。 当然,也可以**先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练**。同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。



* 除了gate之类的地方,需要把输出限制成0-1之外,**尽量不要用sigmoid**,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。

* rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好。

* word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果。
* LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.
* 尽量对数据做shuffle
* 如果你的模型包含全连接层（MLP，多层感知器），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动
* Ensemble：
    ```
    1.同样的参数,不同的初始化方式
    
    2.不同的参数,通过cross-validation,选取最好的几组
    
    3.同样的参数,模型训练的不同阶段，即不同迭代次数的模型。
    
    4.不同的模型,进行线性融合. 例如RNN和传统模型。
    
    ```
# stacking
![img](http://ovapaz2zv.bkt.clouddn.com/stacking-1.jpg)
![img](http://ovapaz2zv.bkt.clouddn.com/stacking-2.jpg)
# 激活函数的特点
    1. 非线性
    2. 几乎处处可微
    3. 计算简单
    4. 非饱和性（saturation）
    5. 单调性（monotonic）：即导数符号不变。单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。
    6. 输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，但这导致了梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出、LSTM里的gate函数。
    7. 接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x>0时为线性。这个性质也让初始化参数范围的推导更为简单。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet和RNN中的LSTM。
    8. 参数少
    9. 归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization。
# 网络结构特点
![img](http://ovapaz2zv.bkt.clouddn.com/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%89%B9%E7%82%B9.jpg)
# Selective Search
![ss](http://ovapaz2zv.bkt.clouddn.com/selective%20search.png)
# NMS（非极大值抑制）
对于选出的一堆bounding box,找到置信度最大的一个例如A，然后其他计算与它的IOU，大于某个阈值的都丢弃掉，然后标记A，再从除去A及丢弃掉的bounding box中重复这一过程、