---
layout: post
title: dl
tags: [诗歌]
category: 文学
mathjax: true
---
## lr遇到缺失值的问题

选择实数 0 来替换所有缺失值，恰好能适用于Logistic 回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值。
$$\omega = \omega + \alpha * x_i $$
如果 dataMatrix 的某个特征对应值为 0，那么该特征的系数将不做更新；另外，由于 Sigmoid(0) = 0.5 ，即它对结果的预测不具有任何倾向性，因此我们上述做法也不会对误差造成任何影响。基于上述原因，将缺失值用 0 代替既可以保留现有数据，也不需要对优化算法进行修改。此外，该数据集中的特征取值一般不为 0，因此在某种意义上说它也满足 “特殊值” 这个要求。

## 生成模型和判别模型
-|生成模型|判别模型
---|---|---
求解目标|先求出联合概率P（X,Y），然后求出条件概率分布P（Y&#124;X）|直接求解P（Y&#124;X）
特点|学习收敛速度更快；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用|直接面对预测，往往学习的准备率更高；由于直接学习P（Y&#124;X）或者f(x)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题
典型模型|朴素贝叶斯、隐马尔可夫|knn,nn,tree-base,lr,svm,boosting，条件随机场

## 感知机学习算法的对偶形式
1. 标准形式
感知机模型为$f(x) = sign({\omega}·x+b)$
* 选取初值${\omega}_0,b_0$
* 在训练集中选取数据$(x_i,y_i)$
* 如果\$$y_i({\omega}·x_i+b)≤0$$

$$\omega\leftarrow\omega+{\eta}y_ix_i$$

$$b{\leftarrow}b+{\eta}y_i$$

* 不断迭代，直到分类全部正确

2. 对偶形式
根据上面不难看出，若假设参数的初始值均为0，最后学习到的参数可以分别表示为：
$$\omega = \sum_{i=1}^N\alpha_iy_ix_i$$
$$b = \sum_{i=1}^N\alpha_iy_i$$
其中$\alpha_i =n_i\eta$(n表示修改了几次)
感知机模型$f(x)=sign(\sum_{j=1}^N\alpha_jy_jx_j·x+b)$
* $\alpha, b$设为0
* 在训练集中选取数据$(x_i,y_i)$
* 如果$y_i(\sum_{j=1}^N\alpha_jy_jx_j·x+b)≤0$
$$\alpha_i \leftarrow\alpha_i+\eta $$
$$b \leftarrow b+\eta y_i $$
对偶形式中训练示例仅以内积的形式出现。为了方便可以将训练集中实例间的内积计算出来并以矩阵形式存储（Gram矩阵）
## 基于梯度的学习


