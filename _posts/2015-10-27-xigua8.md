---
layout: post
title: 西瓜书笔记（三）
tags: [机器学习, 读书笔记, 总结]
category: 技术
mathjax: true
---
>既然开始学习机器学习，那么肯定不能少了经典的西瓜书，作为入门经典教材，讲解的通俗易懂，而且较为系统。工作之余，我也决定读一遍西瓜书，可能大部分内容都有所了解了，这个系列的博客就再总结些自己认为重要的。

# 集成学习
集成学习通过构建并结合多个学习器来完成学习任务，通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能，这对“弱学习器”尤为明显。
## 个体与集成
即使是最简单的“投票”集成，基学习器比随机猜要好（P>0.5），**在假设基学习器误差相互独立的前提下**，随着个体分类器数量的增大，错误率也会下降到0。但个体学习器是为解决同一问题训练出来的，它们显然不可能相互独立，其“准确性”和“多样性”本身存在冲突，因此需要产生并结合“好而不同”的个体学习器。
* 个体学习器间存在强依赖关系，必须串行生成（Boosting）
* 个体学习器间不存在强依赖关系，可同时并行生成（Bagging、RF）

## Boosting
可将弱学习器提升为强学习器的算法：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使先前基学习器做错的样本在后续得到更多关注，然后继续训练**下一个基学习器**。
* AdaBoost

* 从bias-variance分解的角度看，Boosting主要关注降低bias，因此能基于泛化性能相当弱的学习器构建出很强的集成。

## Bagging&RF
为了设法使基学习器尽可能具有较大差异，可对训练样本进行采样，产生若干个不同的子集，再从每个数据子集中训练出一个基学习器。这样由于训练数据不同，基学习器可望具有较大差异。但每个学习器只用到了一小部分训练数据，因此要考虑使用互相交叠的采样子集。
* Bagging
 给定包含m个样本的数据集，我们先随机取出一个放入样本集，然后再把该样本放回初始数据集（下次还能被采到）。这样我们可以采出T个包含m个训练样本的采样集，分别训练出T个基学习器。然后通过“投票法”及“简单平均法”结合。*Bagging更关注降低variance，因此它在不剪枝决策树及NN等易受样本扰动的学习器上效用更为明显。

## RF
RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择：对于基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。**与Bagging“多样性”仅通过样本扰动不同，RF还来自属性扰动。

## 结合策略


* 平均法、加权法
* 投票法（加权投票）
* 学习法(如Stacking):先从初始数据集训练出初级学习器，然后“生成”（交叉验证）一个新数据集，初级学习器的输出被当做样例输入特征。

## 多样性增强
* 数据样本扰动
* 输入属性扰动
* 输出表示扰动：随机改变一些训练样本的标记（翻转法）；将分类输出转化为回归输出（输出调制法）；将原任务拆解为多个可同时求解的子任务（ECOC）
* 算法参数扰动
